{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "18cce651",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dataset\n",
    "import pandas as pd\n",
    "\n",
    "# Data preprocessing\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import re\n",
    "\n",
    "# Model selection\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "import xgboost as xgb\n",
    "\n",
    "\n",
    "# Model evaluation\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "\n",
    "df = pd.read_csv('Downloads/Youtube01-Psy.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "eb627c0d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    huh anyway check out this youtube channel koby...\n",
      "1    hey guys check out my new channel and our firs...\n",
      "2                just for test i have to say murdevcom\n",
      "3         me shaking my sexy ass on my channel enjoy _\n",
      "4                     watchvvtarggvgtwq check this out\n",
      "Name: CONTENT, dtype: object\n"
     ]
    }
   ],
   "source": [
    "# Clean the comments\n",
    "\n",
    "def clean_comment(comment):\n",
    "    # convert to lowercase\n",
    "    comment = comment.lower()\n",
    "\n",
    "    # remove urls\n",
    "    comment = re.sub(r'http\\S+', '', comment)\n",
    "\n",
    "    # remove digits\n",
    "    comment = re.sub(r'\\d+', '', comment)\n",
    "\n",
    "    # remove punctuation\n",
    "    comment = re.sub(r'[^\\w\\s]', '', comment)\n",
    "\n",
    "    # remove extra whitespace\n",
    "    comment = re.sub(r'\\s+', ' ', comment).strip()\n",
    "\n",
    "    return comment\n",
    "\n",
    "# clean the 'comment_text' column\n",
    "df['CONTENT'] = df['CONTENT'].apply(clean_comment)\n",
    "\n",
    "# print the first 5 cleaned comments\n",
    "print(df['CONTENT'].head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "1bebbcd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(df['CONTENT'], df['CLASS'], test_size=0.2, random_state=42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "0f2a413e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature extraction\n",
    "vectorizer = CountVectorizer()\n",
    "X_train = vectorizer.fit_transform(X_train)\n",
    "X_test = vectorizer.transform(X_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "e2fe9e6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Label encoding\n",
    "le = LabelEncoder()\n",
    "y_train = le.fit_transform(y_train)\n",
    "y_test = le.transform(y_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "0132efec",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression()"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Train different models\n",
    "lr_model = LogisticRegression()\n",
    "lr_model.fit(X_train, y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "6913082c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SVC(kernel='linear', probability=True)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "svm_model = SVC(kernel='linear', C=1.0, probability=True)\n",
    "svm_model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "313b117b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DecisionTreeClassifier(max_depth=10)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dt_model = DecisionTreeClassifier(max_depth=10)\n",
    "dt_model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "b2829f25",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression accuracy: 0.8857142857142857\n",
      "Logistic Regression precision: 0.8723404255319149\n",
      "Logistic Regression recall: 0.9534883720930233\n",
      "Logistic Regression f1 score: 0.9111111111111112\n",
      "SVM accuracy: 0.8714285714285714\n",
      "SVM precision: 0.8695652173913043\n",
      "SVM recall: 0.9302325581395349\n",
      "SVM f1 score: 0.898876404494382\n",
      "Decision Tree accuracy: 0.7142857142857143\n",
      "Decision Tree precision: 0.9259259259259259\n",
      "Decision Tree recall: 0.5813953488372093\n",
      "Decision Tree f1 score: 0.7142857142857142\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the models on the testing set\n",
    "lr_preds = lr_model.predict(X_test)\n",
    "svm_preds = svm_model.predict(X_test)\n",
    "dt_preds = dt_model.predict(X_test)\n",
    "\n",
    "lr_accuracy = accuracy_score(y_test, lr_preds)\n",
    "lr_precision = precision_score(y_test, lr_preds)\n",
    "lr_recall = recall_score(y_test, lr_preds)\n",
    "lr_f1_score = f1_score(y_test, lr_preds)\n",
    "\n",
    "svm_accuracy = accuracy_score(y_test, svm_preds)\n",
    "svm_precision = precision_score(y_test, svm_preds)\n",
    "svm_recall = recall_score(y_test, svm_preds)\n",
    "svm_f1_score = f1_score(y_test, svm_preds)\n",
    "\n",
    "dt_accuracy = accuracy_score(y_test, dt_preds)\n",
    "dt_precision = precision_score(y_test, dt_preds)\n",
    "dt_recall = recall_score(y_test, dt_preds)\n",
    "dt_f1_score = f1_score(y_test, dt_preds)\n",
    "\n",
    "print('Logistic Regression accuracy:', lr_accuracy)\n",
    "print('Logistic Regression precision:', lr_precision)\n",
    "print('Logistic Regression recall:', lr_recall)\n",
    "print('Logistic Regression f1 score:', lr_f1_score)\n",
    "\n",
    "print('SVM accuracy:', svm_accuracy)\n",
    "print('SVM precision:', svm_precision)\n",
    "print('SVM recall:', svm_recall)\n",
    "print('SVM f1 score:', svm_f1_score)\n",
    "\n",
    "print('Decision Tree accuracy:', dt_accuracy)\n",
    "print('Decision Tree precision:', dt_precision)\n",
    "print('Decision Tree recall:', dt_recall)\n",
    "print('Decision Tree f1 score:', dt_f1_score)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ed43f30",
   "metadata": {},
   "source": [
    "Overall, the results you got are pretty good. The Logistic Regression model seems to perform the best with an accuracy of 0.885 and an F1 score of 0.911, indicating that it is able to achieve a good balance between precision and recall."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "f47d6aff",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Forest accuracy: 0.8857142857142857\n",
      "Random Forest precision: 0.8723404255319149\n",
      "Random Forest recall: 0.9534883720930233\n",
      "Random Forest f1 score: 0.9111111111111112\n",
      "Gradient Boosting accuracy: 0.8571428571428571\n",
      "Gradient Boosting precision: 0.851063829787234\n",
      "Gradient Boosting recall: 0.9302325581395349\n",
      "Gradient Boosting f1 score: 0.888888888888889\n",
      "XGB accuracy: 0.8714285714285714\n",
      "XGB precision: 0.84\n",
      "XGB recall: 0.9767441860465116\n",
      "XGB f1 score: 0.9032258064516129\n"
     ]
    }
   ],
   "source": [
    "rf_model = RandomForestClassifier(n_estimators=100)\n",
    "rf_model.fit(X_train, y_train)\n",
    "\n",
    "gb_model = GradientBoostingClassifier(n_estimators=100)\n",
    "gb_model.fit(X_train, y_train)\n",
    "\n",
    "xgb_model = xgb.XGBClassifier(n_estimators=100)\n",
    "xgb_model.fit(X_train, y_train)\n",
    "\n",
    "\n",
    "rf_preds = rf_model.predict(X_test)\n",
    "gb_preds = gb_model.predict(X_test)\n",
    "xgb_preds = xgb_model.predict(X_test)\n",
    "\n",
    "\n",
    "rf_accuracy = accuracy_score(y_test, rf_preds)\n",
    "rf_precision = precision_score(y_test, rf_preds)\n",
    "rf_recall = recall_score(y_test, rf_preds)\n",
    "rf_f1_score = f1_score(y_test, rf_preds)\n",
    "\n",
    "gb_accuracy = accuracy_score(y_test, gb_preds)\n",
    "gb_precision = precision_score(y_test, gb_preds)\n",
    "gb_recall = recall_score(y_test, gb_preds)\n",
    "gb_f1_score = f1_score(y_test, gb_preds)\n",
    "\n",
    "xgb_accuracy = accuracy_score(y_test, xgb_preds)\n",
    "xgb_precision = precision_score(y_test, xgb_preds)\n",
    "xgb_recall = recall_score(y_test, xgb_preds)\n",
    "xgb_f1_score = f1_score(y_test,xgb_preds)\n",
    "\n",
    "\n",
    "print('Random Forest accuracy:', rf_accuracy)\n",
    "print('Random Forest precision:', rf_precision)\n",
    "print('Random Forest recall:', rf_recall)\n",
    "print('Random Forest f1 score:', rf_f1_score)\n",
    "\n",
    "\n",
    "print('Gradient Boosting accuracy:', gb_accuracy)\n",
    "print('Gradient Boosting precision:', gb_precision)\n",
    "print('Gradient Boosting recall:', gb_recall)\n",
    "print('Gradient Boosting f1 score:', gb_f1_score)\n",
    "\n",
    "print('XGB accuracy:', xgb_accuracy)\n",
    "print('XGB precision:', xgb_precision)\n",
    "print('XGB recall:', xgb_recall)\n",
    "print('XGB f1 score:', xgb_f1_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "333f57ec",
   "metadata": {},
   "source": [
    "It seems like the Random Forest model is performing the best in terms of accuracy, precision, recall, and f1 score. However, the XGBoost model has the highest recall score, indicating that it correctly identifies most of the spam comments."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
